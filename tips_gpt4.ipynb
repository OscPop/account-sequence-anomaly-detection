{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data: 6873\n",
      "Ex: [18, 18, 18, 18, 1, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/h0v_s94x231gxb7bq6h6wtpm0000gn/T/ipykernel_4043/3189928224.py:90: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525682339/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  loss = criterion(outputs.reshape(-1, vocab_size)[mask], targets[:, 1:].reshape(-1)[mask])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3572855591773987\n",
      "Epoch 2/10, Loss: 0.5234084725379944\n",
      "Epoch 3/10, Loss: 0.02439974993467331\n",
      "Epoch 4/10, Loss: 0.11817251145839691\n",
      "Epoch 5/10, Loss: 0.012505429796874523\n",
      "Epoch 6/10, Loss: 0.007246554363518953\n",
      "Epoch 7/10, Loss: 0.0849609524011612\n",
      "Epoch 8/10, Loss: 0.05347998067736626\n",
      "Epoch 9/10, Loss: 0.2576196789741516\n",
      "Epoch 10/10, Loss: 0.026256121695041656\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare the data\n",
    "class AccountSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len=16):\n",
    "        self.sequences = sequences\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_sequence = sequence[:-1]\n",
    "        target_sequence = sequence[1:]\n",
    "\n",
    "        # Pad input and target sequences to max_len\n",
    "        input_sequence = input_sequence + [padding_token] * (self.max_len - len(input_sequence))\n",
    "        target_sequence = target_sequence + [padding_token] * (self.max_len - len(target_sequence))\n",
    "\n",
    "        return torch.tensor(input_sequence, dtype=torch.long), torch.tensor(target_sequence, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Read encoded_sequences from file. 0 -> 101\n",
    "objects = []\n",
    "with (open(\"encoded_sequences.pkl\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            objects.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "encoded_sequences = objects[0]\n",
    "print(f\"Length of data: {len(encoded_sequences)}\")\n",
    "print(f\"Ex: {encoded_sequences[0]}\")\n",
    "\n",
    "#sequences = [[1, 29, 34, 12, 45], ...]\n",
    "max_account_number = max([max(seq) for seq in encoded_sequences]) # 101\n",
    "padding_token = max_account_number + 1  # 102\n",
    "dataset = AccountSequenceDataset(encoded_sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 2. Define the transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src).transpose(0, 1)  # Transpose to (sequence_length, batch_size, d_model)\n",
    "        tgt = self.embedding(tgt).transpose(0, 1)  # Transpose to (sequence_length, batch_size, d_model)\n",
    "        x = self.transformer(src, tgt)\n",
    "        x = self.fc(x)\n",
    "        return x.transpose(0, 1)  # Transpose back to (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = padding_token + 1          # 103 (0->101 + 102)\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_layers)\n",
    "\n",
    "# 3. Train the model\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"mps\"\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Remove the last element from the target sequence before passing it to the model\n",
    "        outputs = model(inputs, targets[:, :-1])\n",
    "\n",
    "        # Apply masking to ignore padding tokens\n",
    "        mask = (targets[:, 1:] != padding_token).view(-1)  # Create a mask for non-zero elements\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size)[mask], targets[:, 1:].reshape(-1)[mask])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'models/2023-04-13_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict next element in a given sequence\n",
    "def predict_next_element(model, input_sequence, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_sequence = torch.tensor(input_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        input_length = input_sequence.size(1)\n",
    "\n",
    "        # Initialize target sequence with zeros\n",
    "        tgt = torch.zeros(1, input_length, dtype=torch.long).to(device)\n",
    "\n",
    "        # Autoregressive generation\n",
    "        for i in range(input_length):\n",
    "            output = model(input_sequence, tgt)\n",
    "            output = output.argmax(dim=2)  # Get the index of the predicted element\n",
    "            if i + 1 < input_length:\n",
    "                tgt[:, i + 1] = output[:, i]\n",
    "\n",
    "        predicted_next_element = output[0, -1].item()\n",
    "    return predicted_next_element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next element in sequence [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0] is 75\n"
     ]
    }
   ],
   "source": [
    "# Provide an input sequence and predict next element\n",
    "input_sequence = []\n",
    "predicted_next_element = predict_next_element(model, input_sequence, device)\n",
    "print(f\"Next element in sequence {input_sequence} is {predicted_next_element}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find anomalies\n",
    "\n",
    "def sequence_log_prob(model, input_sequence, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_sequence = torch.tensor(input_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        input_length = input_sequence.size(1)\n",
    "\n",
    "        # Initialize target sequence with zeros\n",
    "        tgt = torch.zeros(1, input_length, dtype=torch.long).to(device)\n",
    "\n",
    "        log_probs = []\n",
    "        for i in range(input_length - 1):\n",
    "            output = model(input_sequence, tgt)\n",
    "            prob = torch.softmax(output, dim=2)  # Convert logits to probabilities\n",
    "            log_prob = torch.log(prob[0, i, input_sequence[0, i + 1]])\n",
    "            log_probs.append(log_prob.item())\n",
    "\n",
    "            tgt[:, i + 1] = input_sequence[:, i + 1]\n",
    "\n",
    "    return np.mean(log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities and then classify outliers\n",
    "sequences_log_probs = [sequence_log_prob(model, seq, device) for seq in encoded_sequences]\n",
    "\n",
    "# Calculate the threshold (e.g., mean minus two standard deviations)\n",
    "threshold = np.mean(sequences_log_probs) - 2 * np.std(sequences_log_probs)\n",
    "\n",
    "outlier_sequences = [seq for seq, log_prob in zip(encoded_sequences, sequences_log_probs) if log_prob < threshold]\n",
    "\n",
    "print(\"Outlier sequences:\")\n",
    "for outlier_seq in outlier_sequences:\n",
    "    print(outlier_seq)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fail_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
