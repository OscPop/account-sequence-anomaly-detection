{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data: 6873\n",
      "Ex: [18, 18, 18, 18, 1, 6]\n",
      "Epoch 1/100, Loss: 0.01813587360084057\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs, targets[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     81\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vocab_size), targets[:, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m---> 82\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     83\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     85\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\OscarLarsen\\miniconda3\\envs\\fail_env\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\OscarLarsen\\miniconda3\\envs\\fail_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "\n",
    "# 1. Prepare the data\n",
    "class AccountSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len=16):\n",
    "        self.sequences = sequences\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_sequence = sequence[:-1]\n",
    "        target_sequence = sequence[1:]\n",
    "\n",
    "        # Pad input and target sequences to max_len\n",
    "        input_sequence = input_sequence + [0] * (self.max_len - len(input_sequence))\n",
    "        target_sequence = target_sequence + [0] * (self.max_len - len(target_sequence))\n",
    "\n",
    "        return torch.tensor(input_sequence, dtype=torch.long), torch.tensor(target_sequence, dtype=torch.long)\n",
    "\n",
    "# Replace with your 6000 sequences\n",
    "\n",
    "objects = []\n",
    "# Encoded sequences:  0 -> 101\n",
    "with (open(\"encoded_sequences.pkl\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            objects.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "encoded_sequences = objects[0]\n",
    "print(f\"Length of data: {len(encoded_sequences)}\")\n",
    "print(f\"Ex: {encoded_sequences[0]}\")\n",
    "\n",
    "#sequences = [[1, 29, 34, 12, 45], ...]\n",
    "dataset = AccountSequenceDataset(encoded_sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 2. Define the transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src).transpose(0, 1)  # Transpose to (sequence_length, batch_size, d_model)\n",
    "        tgt = self.embedding(tgt).transpose(0, 1)  # Transpose to (sequence_length, batch_size, d_model)\n",
    "        x = self.transformer(src, tgt)\n",
    "        x = self.fc(x)\n",
    "        return x.transpose(0, 1)  # Transpose back to (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = max([max(seq) for seq in encoded_sequences]) + 1  # Assuming account numbers start from 0\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_layers)\n",
    "\n",
    "# 3. Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Remove the last element from the target sequence before passing it to the model\n",
    "        outputs = model(inputs, targets[:, :-1])\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), targets[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fail_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
